{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPGVVMTFAzWM6w38OuL9yWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohmreborn/rnn-lstm-gru-pytorch-from-scratch/blob/main/explain_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA4qtkioYaTw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
        "        super(RNNCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.nonlinearity = nonlinearity\n",
        "        if self.nonlinearity not in [\"tanh\", \"relu\"]:\n",
        "            raise ValueError(\"Invalid nonlinearity selected for RNN.\")\n",
        "\n",
        "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        # Inputs:\n",
        "        #       input: of shape (batch_size, input_size)\n",
        "        #       hx: of shape (batch_size, hidden_size)\n",
        "        # Output:\n",
        "        #       hy: of shape (batch_size, hidden_size)\n",
        "\n",
        "        if hx is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size)\n",
        "\n",
        "        hy = (self.x2h(input) + self.h2h(hx))\n",
        "\n",
        "        if self.nonlinearity == \"tanh\":\n",
        "            hy = torch.tanh(hy)\n",
        "        else:\n",
        "            hy = torch.relu(hy)\n",
        "\n",
        "        return hy"
      ],
      "metadata": {
        "id": "OSUcrmOkYiNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size, activation='tanh'):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.rnn_cell_list = nn.ModuleList()\n",
        "\n",
        "        if activation == 'tanh':\n",
        "            self.rnn_cell_list.append(RNNCell(self.input_size,\n",
        "                                                   self.hidden_size,\n",
        "                                                   self.bias,\n",
        "                                                   \"tanh\"))\n",
        "            for l in range(1, self.num_layers):\n",
        "                self.rnn_cell_list.append(RNNCell(self.hidden_size,\n",
        "                                                       self.hidden_size,\n",
        "                                                       self.bias,\n",
        "                                                       \"tanh\"))\n",
        "\n",
        "        elif activation == 'relu':\n",
        "            self.rnn_cell_list.append(RNNCell(self.input_size,\n",
        "                                                   self.hidden_size,\n",
        "                                                   self.bias,\n",
        "                                                   \"relu\"))\n",
        "            for l in range(1, self.num_layers):\n",
        "                self.rnn_cell_list.append(RNNCell(self.hidden_size,\n",
        "                                                   self.hidden_size,\n",
        "                                                   self.bias,\n",
        "                                                   \"relu\"))\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation.\")\n",
        "\n",
        "        # self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        # Input of shape (batch_size, seqence length, input_size)\n",
        "        #\n",
        "        # Output of shape (batch_size, output_size)\n",
        "\n",
        "        if hx is None:\n",
        "            if torch.cuda.is_available():\n",
        "                h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda()\n",
        "            else:\n",
        "                h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
        "\n",
        "        else:\n",
        "             h0 = hx\n",
        "\n",
        "        outs = []\n",
        "\n",
        "        hidden = list()\n",
        "        for layer in range(self.num_layers):\n",
        "            hidden.append(h0[layer, :, :])\n",
        "\n",
        "        for t in range(input.size(1)):\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "\n",
        "                if layer == 0:\n",
        "                    hidden_l = self.rnn_cell_list[layer](input[:, t, :], hidden[layer])\n",
        "                else:\n",
        "                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1],hidden[layer])\n",
        "                hidden[layer] = hidden_l\n",
        "            outs.append(hidden_l)\n",
        "\n",
        "        # Take only last time step. Modify for seq to seq\n",
        "        # out = outs[-1].squeeze()\n",
        "\n",
        "        # out = self.fc(out)\n",
        "\n",
        "\n",
        "        return torch.stack(outs),hidden"
      ],
      "metadata": {
        "id": "IfiuLCdhYjyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleRNN(10, 20, 2,True,20)"
      ],
      "metadata": {
        "id": "vt66Bk8rYmCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = {'weight_ih_l0':'rnn_cell_list.0.x2h.weight',\n",
        " 'weight_hh_l0':'rnn_cell_list.0.h2h.weight',\n",
        " 'bias_ih_l0':'rnn_cell_list.0.x2h.bias',\n",
        " 'bias_hh_l0':'rnn_cell_list.0.h2h.bias',\n",
        " 'weight_ih_l1':'rnn_cell_list.1.x2h.weight',\n",
        " 'weight_hh_l1':'rnn_cell_list.1.h2h.weight',\n",
        " 'bias_ih_l1':'rnn_cell_list.1.x2h.bias',\n",
        " 'bias_hh_l1':'rnn_cell_list.1.h2h.bias'\n",
        " }"
      ],
      "metadata": {
        "id": "TUJJRIgwYnR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = model.state_dict()\n",
        "from collections import OrderedDict\n",
        "\n",
        "d = OrderedDict()\n",
        "d['weight_ih_l0'] = state['rnn_cell_list.0.x2h.weight']\n",
        "d['weight_hh_l0'] = state['rnn_cell_list.0.h2h.weight']\n",
        "d['bias_ih_l0'] = state['rnn_cell_list.0.x2h.bias']\n",
        "d['bias_hh_l0'] = state['rnn_cell_list.0.h2h.bias']\n",
        "d['weight_ih_l1'] = state['rnn_cell_list.1.x2h.weight']\n",
        "d['weight_hh_l1'] = state['rnn_cell_list.1.h2h.weight']\n",
        "d['bias_ih_l1'] = state['rnn_cell_list.1.x2h.bias']\n",
        "d['bias_hh_l1'] = state['rnn_cell_list.1.h2h.bias']\n",
        "\n",
        "d"
      ],
      "metadata": {
        "id": "ApGkYde5YpDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.RNN(10, 20, 2,batch_first=True)\n",
        "rnn.load_state_dict(d)\n",
        "rnn"
      ],
      "metadata": {
        "id": "iccEmylzYrHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 5, 10)\n",
        "h0 = torch.randn(2, 3, 20)"
      ],
      "metadata": {
        "id": "Vd7vxMDKYsXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, hn = rnn(input, h0)\n",
        "o,h = model(input,h0)\n"
      ],
      "metadata": {
        "id": "MeAytT_6Ytwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hn"
      ],
      "metadata": {
        "id": "0Z_Yl9JxYvXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack(h)"
      ],
      "metadata": {
        "id": "xJsAMSusYw-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKyUGNIRZncM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}